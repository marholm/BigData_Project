{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marholm/BigData_Project/blob/main/BigData_IMDB_Team19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_qCfsl-GIH9"
      },
      "source": [
        "# 0. Set up environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll_9eI8cD-Y_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a4d15c-db18-499c-d67e-3e2803351b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 13 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 42.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=e3aa8c111b763b502eb2cbf13f605c3b13d5876cc6ed8f91e264cb5f7c5529fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Collecting chart-studio\n",
            "  Downloading chart_studio-1.1.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting retrying>=1.3.3\n",
            "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from chart-studio) (5.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from chart-studio) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from chart-studio) (2.23.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->chart-studio) (8.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->chart-studio) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->chart-studio) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->chart-studio) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->chart-studio) (1.24.3)\n",
            "Building wheels for collected packages: retrying\n",
            "  Building wheel for retrying (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11447 sha256=91ca9f997baa0d958dac772bdb595fa7504d3c0645dd6fec5a46332948ca3e19\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/8d/8d/f6af3f7f9eea3553bc2fe6d53e4b287dad18b06a861ac56ddf\n",
            "Successfully built retrying\n",
            "Installing collected packages: retrying, chart-studio\n",
            "Successfully installed chart-studio-1.1.0 retrying-1.3.3\n"
          ]
        }
      ],
      "source": [
        "# Required installations\n",
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "# !pip install duckdb\n",
        "# !pip install numpy\n",
        "!pip install pandas\n",
        "# !pip install fitter\n",
        "!pip install chart-studio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLwehhb7Eana"
      },
      "outputs": [],
      "source": [
        "# import os       #importing os to set environment variable\n",
        "# def install_java():\n",
        "#   !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "#   os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "#   !java -version       #check java version\n",
        "# install_java()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BM4em8bu7FM"
      },
      "outputs": [],
      "source": [
        "# when errors occur in above method: !apt-get update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX6ozSjoEobs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a8c8806-35c7-43f0-d4c7-0a0e1d44531b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: SPARK_HOME=/usr/local/lib/python3.7/dist-packages/pyspark\n"
          ]
        }
      ],
      "source": [
        "%env SPARK_HOME=/usr/local/lib/python3.7/dist-packages/pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIUEO3pIErir"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLNJj_ACBa_1"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql import functions\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local\") \\\n",
        "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
        "    .config(\"spark.executor.memory\", \"70g\") \\\n",
        "    .config(\"spark.driver.memory\", \"50g\") \\\n",
        "    .config(\"spark.sql.analyzer.failAmbiguousSelfJoin\", False) \\\n",
        "    .config(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
        "    .config(\"spark.memory.offHeap.size\",\"10g\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqOCMhp-8c6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9257cedc-8bdd-49b7-acf6-41aaad52cf5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4Dr5dLqAvMS"
      },
      "source": [
        "# 1. Data Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfvCXx-CYMLq"
      },
      "source": [
        "##  1.1 Data Ingestion\n",
        "\n",
        "Collection of original data from Github, persisted without any transformation. \n",
        "* Training data (CSV)\n",
        "* Testing and validation data (CSV)\n",
        "* Writers and Directors (JSON)\n",
        "- External data from: https://www.kaggle.com/rounakbanik/the-movies-dataset (CSV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tg00tKq9bxv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76b4310f-685b-4e71-a62e-6cc80d670910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'big-data-course-2022-projects'...\n",
            "remote: Enumerating objects: 127, done.\u001b[K\n",
            "remote: Counting objects: 100% (127/127), done.\u001b[K\n",
            "remote: Compressing objects: 100% (108/108), done.\u001b[K\n",
            "remote: Total 127 (delta 37), reused 49 (delta 5), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (127/127), 5.88 MiB | 17.54 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/schelterlabs/big-data-course-2022-projects.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S42pct__Gev7"
      },
      "outputs": [],
      "source": [
        "df_train = spark.read.options(header='True', inferSchema='True', delimiter=',')\\\n",
        "    .csv(\"big-data-course-2022-projects/imdb/train-*.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lec-WHzqPyHe",
        "outputId": "a9a694cd-7fa9-44dc-dcac-55e1ed0e6a61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7959"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uL5PlVxbBeh"
      },
      "outputs": [],
      "source": [
        "df_test = spark.read.options(header='True', inferSchema='True', delimiter=',')\\\n",
        "    .csv(\"big-data-course-2022-projects/imdb/test_hidden.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPH_iZf0sa1a"
      },
      "outputs": [],
      "source": [
        "df_validation = spark.read.options(header='True', inferSchema='True', delimiter=',')\\\n",
        "    .csv(\"big-data-course-2022-projects/imdb/validation_hidden.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4b2TwEcbYRX"
      },
      "outputs": [],
      "source": [
        "df_writers = spark.read.json(\"big-data-course-2022-projects/imdb/writing.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_writers.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1YVe6kjQfjv",
        "outputId": "5485864f-d93f-4782-fb6f-97f545040358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22428"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCPhGb7abTji"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_directors = pd.read_json(\"big-data-course-2022-projects/imdb/directing.json\")\n",
        "df_directors = spark.createDataFrame(df_directors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_directors.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYSsDeL9QiO1",
        "outputId": "c02e4ba8-c74a-4447-8edd-8c1b7df1297a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11162"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.1 Exploratory analysis\n",
        "\n",
        "Exploring original data "
      ],
      "metadata": {
        "id": "4lOZYCsIcs_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from chart_studio import plotly\n",
        "# import plotly.offline as py\n",
        "# import plotly.graph_objs as go\n",
        "\n",
        "# # Info on https://plotly.com/python/v3/apache-spark/\n",
        "# data = [go.Histogram(x=df_train.toPandas()['numVotes'])]\n",
        "# py.iplot(data)\n",
        "\n",
        "# Find outliers startYear\n",
        "# data = [go.Histogram(x=df_train.toPandas()['startYear'])]\n",
        "# py.iplot(data)"
      ],
      "metadata": {
        "id": "SgrPsvQKbH1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pyspark.sql.functions as F\n",
        "\n",
        "# plot_labels = df_train.groupBy('label')\\\n",
        "#               .agg(\n",
        "#                   F.count('label').alias(\"label count\"), \\\n",
        "#                   F.avg(\"numVotes\").alias(\"avg vote per label\")\\\n",
        "#               )\n",
        "\n",
        "# plot_labels.toPandas()"
      ],
      "metadata": {
        "id": "wtcyp2s7dDNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Ew9cdvZ0YS"
      },
      "source": [
        "## 1.2 Data Preperation\n",
        "* Data Cleaning\n",
        "* Join writers and directors on training df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9WI3RgxHjtF"
      },
      "source": [
        "### 1.2.1 Data Cleaning\n",
        "* Replace empty fields in `numVotes` with the integer 0.\n",
        "* Replace special characters (e.g., accented characters such as  à and é) with ASCII characters.\n",
        "* Replace empty `originalTitle` fields with a copy of `primaryTitle` and the other way around. \n",
        "* Replace `endYear` cells with `\\N` with the value from `startYear`.\n",
        "* Check outliers. Look for movies before 1900 for example.\n",
        "* Capitalization rules. Chose a specific format.**bold text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2XXT9wYE1uC"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "\n",
        "def replace_empty_with_nan(df, col=None, t=None):\n",
        "    df = df.withColumn(col, \\\n",
        "                      F.when(df[col].isNull(), np.nan)\\\n",
        "                      .otherwise(df[col])\\\n",
        "                      .cast(t))\n",
        "    return df\n",
        "\n",
        "\n",
        "def replace_empty_with_zero(df, col=None, t=None):\n",
        "    df = df.withColumn(col, \\\n",
        "                       F.when(df[col].isNull(), 0)\\\n",
        "                       .otherwise(df[col])\\\n",
        "                       .cast(t))\n",
        "    return df\n",
        "\n",
        "def replace_special_characters(df, col=None):\n",
        "    my_udf_2 = F.udf(lambda x: unicodedata.normalize('NFD', x)\\\n",
        "                     .encode('ascii', 'ignore').decode(\"utf-8\"))\n",
        "    df = df.withColumn(col, my_udf_2(col))\n",
        "\n",
        "    return df\n",
        "\n",
        "def replace_empty_with_col(df, col=None, col1=None, t=None):\n",
        "    df = df.withColumn(col, \\\n",
        "                      F.when(df[col].isNull(), df[col1])\\\n",
        "                      .otherwise(df[col])\\\n",
        "                      .cast(t))\n",
        "    return df\n",
        "\n",
        "def replace_empty_years(df):\n",
        "    df = df.withColumn(\"endYear\", \\\n",
        "                      F.when(df[\"endYear\"] == r\"\\N\", df[\"startYear\"])\\\n",
        "                      .otherwise(df[\"endYear\"])\\\n",
        "                      .cast(T.IntegerType()))\n",
        "  \n",
        "    df = df.withColumn(\"startYear\", \\\n",
        "                      F.when(df[\"startYear\"] == r\"\\N\", df[\"endYear\"])\\\n",
        "                      .otherwise(df[\"startYear\"])\\\n",
        "                      .cast(T.IntegerType()))\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_df(df):\n",
        "    \"\"\" Functions calls the different cleaning functions. Here you can experiment\"\"\"\n",
        "\n",
        "    df = replace_empty_with_nan(df, \"runtimeMinutes\", T.IntegerType())\n",
        "    df = replace_empty_with_nan(df, \"numVotes\", T.IntegerType())\n",
        "\n",
        "    df = replace_empty_with_col(df, \"originalTitle\", \"primaryTitle\", T.StringType())\n",
        "    df = replace_empty_with_col(df, \"primaryTitle\", \"originalTitle\", T.StringType())\n",
        "    df = replace_empty_with_col(df, \"numVotes\", \"numVotes\", T.IntegerType())\n",
        "\n",
        "    df = replace_special_characters(df, \"primaryTitle\")\n",
        "    df = replace_special_characters(df, 'originalTitle')\n",
        "\n",
        "    df = replace_empty_years(df)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "IMBDuHs54Oe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGLdZz-7jzn6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "620234f3-68e8-4969-ecd6-6322e6894326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------------+-------------+---------+-------+--------------+--------+-----+\n",
            "|_c0|tconst|primaryTitle|originalTitle|startYear|endYear|runtimeMinutes|numVotes|label|\n",
            "+---+------+------------+-------------+---------+-------+--------------+--------+-----+\n",
            "|  0|     0|           0|            0|        0|      0|            13|       0|    0|\n",
            "+---+------+------------+-------------+---------+-------+--------------+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Clean TRAIN data\n",
        "df_train = clean_df(df_train)\n",
        "# Check for invalid values\n",
        "df_train.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df_train.columns]).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lEJG2CTqsxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b71a540b-e2be-4650-a518-990f93473987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------------+-------------+---------+-------+--------------+--------+\n",
            "|_c0|tconst|primaryTitle|originalTitle|startYear|endYear|runtimeMinutes|numVotes|\n",
            "+---+------+------------+-------------+---------+-------+--------------+--------+\n",
            "|  0|     0|           0|            0|        0|      0|             1|       0|\n",
            "+---+------+------------+-------------+---------+-------+--------------+--------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1086"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Clean TEST\n",
        "df_test = clean_df(df_test)\n",
        "# Check if empty values\n",
        "df_test.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df_test.columns]).show() \n",
        "df_test.count() # 1086 rows"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean VALID\n",
        "df_validation = clean_df(df_validation)\n",
        "# Check for invalid values\n",
        "df_validation.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df_validation.columns]).show()\n",
        "df_validation.count() #955 rows"
      ],
      "metadata": {
        "id": "pwPPk5XoRnbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2f53fc4-cf56-43a5-c030-122efff4486b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------------+-------------+---------+-------+--------------+--------+\n",
            "|_c0|tconst|primaryTitle|originalTitle|startYear|endYear|runtimeMinutes|numVotes|\n",
            "+---+------+------------+-------------+---------+-------+--------------+--------+\n",
            "|  0|     0|           0|            0|        0|      0|             2|       0|\n",
            "+---+------+------------+-------------+---------+-------+--------------+--------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "955"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Outliers\n",
        "\n",
        "@ Marianne, maybe we can look together what we keep and what we can remove from this part:)"
      ],
      "metadata": {
        "id": "Yx-Tm9WJyfXz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WQlFz8HZIWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f94bf2-4ef8-4dc2-eeb8-a6c17d9205f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Median Absolute Deviation(numVotes):  2865.8700883713286\n"
          ]
        }
      ],
      "source": [
        "# Calculate important metrics\n",
        "from pyspark.sql.functions import mean, stddev, col\n",
        "from statsmodels import robust\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Task: Outliers\n",
        "numVotes_show = df_train.select(\"numVotes\")\n",
        "\n",
        "# Mean\n",
        "numVotes_mean = df_train.select(mean(\"numVotes\"))\n",
        "\n",
        "# Standard Deviation (average deviation from the mean)\n",
        "numVotes_std = df_train.select(stddev(\"numVotes\"))\n",
        "\n",
        "# Median Absolute Deviation (MAD)\n",
        "# 1. Put values of numVotes in a list \n",
        "numVotesList = df_train.select(\"numVotes\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# 2. Using numpy MAD function \n",
        "numVotes_mad = robust.mad(numVotesList)  # add c=1 to eliminate scaling factor\n",
        "print('Median Absolute Deviation(numVotes): ', numVotes_mad)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit, avg\n",
        "from pyspark.sql.window import Window\n",
        "# x = df_train.groupBy('startYear')\\\n",
        "#               .agg(F.avg(\"numVotes\").alias(\"avg vote per year\")\\\n",
        "#               )\n",
        "\n",
        "\n",
        "# # # doesnt work with the function\n",
        "# # # how to add a new column \n",
        "# # .alias('Mean_column'\n",
        "# # Create variable -> Calculate average -> Use .lit and put variable in it\n",
        "# z = F.avg('numVotes')\n",
        "# value = z\n",
        "# print(value)\n",
        "# # i = F.lit(F.mean('numVotes'))\n",
        "# # g = F.avg(df_train['numVotes'])\n",
        "# # h = lit(F.mean(df_train['numVotes']))\n",
        "# # y = df_train.withColumn(('numVotes'), lit(120))\n",
        "# #df2 = df_train.select(col(\"numVotes\"),lit(numVotes_mean).alias(\"New_Column\"))\n",
        "\n",
        "# #mean_ = df_train.groupBy().avg(\"numVotes\").take(1)[0][0]\n",
        "# #df_train.withColumn(\"test\", lit(mean)).show()\n",
        "\n",
        "# #df_train.withColumn(\"mean\", lit(df_train.select(avg(\"numVotes\") AS (\"temp\")).first().getAs(\"temp\"))).show()\n",
        "\n",
        "# #df2.show()\n",
        "#partitionBy('numVotes')\n",
        "windowSpec = Window.orderBy(\"directors\")\n",
        "# .alias('newshit') -> using alias had no effect\n",
        "df_train.withColumn(\"tryAvgCol\", lit(F.avg(\"numVotes\").over(windowSpec)))  \n",
        "\n",
        "# ok super basic: 1. ny kolonne -> 2. lit(mean) -> 3. when 0-> lit\n",
        "# df_train = df_train.withColumn(\"newAvgCol\", lit(1000))\n",
        "\n",
        "# df_train = df_train.withColumn('numVotes', \\\n",
        "#                     F.when(df_train['numVotes'].isNull(), df_train[\"newAvgCol\"])\\\n",
        "#                     .otherwise(df_train['numVotes'])\\\n",
        "#                     .cast(T.IntegerType())).show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A_BrM245bvoJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "outputId": "4967075d-e79d-4760-b770-1e8d4d26959f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-5ff1127a08b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mwindowSpec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"directors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# .alias('newshit') -> using alias had no effect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tryAvgCol\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numVotes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindowSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# ok super basic: 1. ny kolonne -> 2. lit(mean) -> 3. when 0-> lit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'directors' given input columns: [_c0, endYear, label, numVotes, originalTitle, primaryTitle, runtimeMinutes, startYear, tconst];\n'Project [_c0#16, tconst#17, primaryTitle#192, originalTitle#203, startYear#223, endYear#213, runtimeMinutes#141, numVotes#181, label#24, avg(numVotes#181) windowspecdefinition('directors ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS tryAvgCol#786]\n+- Project [_c0#16, tconst#17, primaryTitle#192, originalTitle#203, cast(CASE WHEN (startYear#20 = \\N) THEN cast(endYear#213 as string) ELSE startYear#20 END as int) AS startYear#223, endYear#213, runtimeMinutes#141, numVotes#181, label#24]\n   +- Project [_c0#16, tconst#17, primaryTitle#192, originalTitle#203, startYear#20, cast(CASE WHEN (endYear#21 = \\N) THEN startYear#20 ELSE endYear#21 END as int) AS endYear#213, runtimeMinutes#141, numVotes#181, label#24]\n      +- Project [_c0#16, tconst#17, primaryTitle#192, <lambda>(originalTitle#161) AS originalTitle#203, startYear#20, endYear#21, runtimeMinutes#141, numVotes#181, label#24]\n         +- Project [_c0#16, tconst#17, <lambda>(primaryTitle#171) AS primaryTitle#192, originalTitle#161, startYear#20, endYear#21, runtimeMinutes#141, numVotes#181, label#24]\n            +- Project [_c0#16, tconst#17, primaryTitle#171, originalTitle#161, startYear#20, endYear#21, runtimeMinutes#141, cast(CASE WHEN isnull(numVotes#151) THEN numVotes#151 ELSE numVotes#151 END as int) AS numVotes#181, label#24]\n               +- Project [_c0#16, tconst#17, cast(CASE WHEN isnull(primaryTitle#18) THEN originalTitle#161 ELSE primaryTitle#18 END as string) AS primaryTitle#171, originalTitle#161, startYear#20, endYear#21, runtimeMinutes#141, numVotes#151, label#24]\n                  +- Project [_c0#16, tconst#17, primaryTitle#18, cast(CASE WHEN isnull(originalTitle#19) THEN primaryTitle#18 ELSE originalTitle#19 END as string) AS originalTitle#161, startYear#20, endYear#21, runtimeMinutes#141, numVotes#151, label#24]\n                     +- Project [_c0#16, tconst#17, primaryTitle#18, originalTitle#19, startYear#20, endYear#21, runtimeMinutes#141, cast(CASE WHEN isnull(numVotes#23) THEN NaN ELSE numVotes#23 END as int) AS numVotes#151, label#24]\n                        +- Project [_c0#16, tconst#17, primaryTitle#18, originalTitle#19, startYear#20, endYear#21, cast(CASE WHEN isnull(runtimeMinutes#22) THEN cast(NaN as string) ELSE runtimeMinutes#22 END as int) AS runtimeMinutes#141, numVotes#23, label#24]\n                           +- Relation [_c0#16,tconst#17,primaryTitle#18,originalTitle#19,startYear#20,endYear#21,runtimeMinutes#22,numVotes#23,label#24] csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter Outliers\n",
        "# Filter rows by movie-date (1888 < movie_date < 2022)\n",
        "df_train.filter(df_train['endYear'] < 1888)\n",
        "df_train.filter(df_train['endYear'] > 2022)\n",
        "\n",
        "# Replace numVotes==0 with the mean THE ERROR IS REPLACE PART!\n",
        "# myudf = F.udf(lambda x: F.avg if x is None else x)\n",
        "\n",
        "# df_train = df_train.withColumn('numVotes',\\\n",
        "#                                 F.when(df_train['numVotes'] == np.isnan(),\\\n",
        "#                                        df_train.agg(F.avg(c) for c in df_train.columns))\\\n",
        "#                                 .otherwise(df_train['numVotes'])).show()\n",
        "\n",
        "\n",
        "# df_train = df_train.withColumn('numVotes',\\\n",
        "#                                 F.when(df_train['numVotes'].isnan(), myudf)\\\n",
        "#                                 .otherwise(df_train['numVotes'])).show()\n",
        "\n",
        "#df_train.withColumn('numVotes',\\\n",
        "#                    F.when('numVotes').isNull(), mean('numVotes')\\\n",
        "#                    .otherwise(col('numVotes')))\n",
        "# Filter outliers based on MAD\n",
        "# df_train.filter((df_train['numVotes'] > (mad*10)) | (df_train['numVotes'] < (mad/10))).show()\n",
        "\n",
        "# Filter outliers deviating more than 2 times from the mad (or something)\n",
        "# df_train = df_train.withColumn('numVotes',\\\n",
        "#                    F.where(numVotes_mad/2 < df_train['numVotes'] < numVotes_mad*2))\\\n",
        "#                    .show()\n",
        "\n",
        "\n",
        "# # MAD outlier filter\n",
        "# MADdf = df.groupby('genre')\n",
        "# .agg(F.expr('percentile(duration, array(0.5))')[0]\n",
        "#      .alias('duration_median')).join(df, \"genre\", \"left\")\n",
        "#      .withColumn(\"duration_difference_median\", F.abs(F.col('duration')-F.col('duration_median')))\n",
        "#      .groupby('genre', 'duration_median').agg(F.expr('percentile(duration_difference_median, array(0.5))')[0]\n",
        "#                                               .alias('median_absolute_difference'))\n",
        "\n",
        "# # source: https://toritompkins.co.uk/identifying-data-outliers-in-apache-spark-3-0/\n",
        "# mad_df = df_train.groupBy('numVotes').agg(F.expr('percentile(duration, array(0.5))')[0]\\\n",
        "#      .alias('duration_median').join(df_train, 'numVotes', 'left')\\\n",
        "#      .withColumn('duration_difference_median', F.abs(F.col('duration')-F.col('duration_median')))\\\n",
        "#      .groupBy('numVotes', 'duration_median').agg(F.expr('percentile(duration_difference_median, array(0.5))')[0]\\\n",
        "#                                                  .alias('median_absolute_difference')))\n",
        "\n",
        "# outliersremoved = df_train.join(mad_df, \"numVotes\", \"left\")\\\n",
        "# .filter(F.abs(F.col(\"duration\")-F.col(\"duration_median\")) <= (F.col(\"mean_absolute_difference\")*3))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vmDlp6oo4YI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2.3 Writers and Directors"
      ],
      "metadata": {
        "id": "gNGSpVYNa2we"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiUNbDPzCd8r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import re\n",
        "from itertools import chain\n",
        "\n",
        "movie_writers = {}\n",
        "movie_directors = {}\n",
        "\n",
        "# Read external data\n",
        "with open(\"drive/MyDrive/Bigdata-Grp19/ext data/credits.csv\", 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader: # every row contains crew of 1 movie \n",
        "      crew = re.findall(r\"\\{([^}]+)\\}\", row['crew']) # find { crew }\n",
        "\n",
        "      writers = []\n",
        "      directors = []\n",
        "\n",
        "      for people in crew:\n",
        "        name = re.search(r\"(?<='name': )(..*?),\", people)\n",
        "        \n",
        "        if 'Director' in people:\n",
        "            director = name.group(1).replace(\"'\", \"\")\n",
        "            directors.append(director)\n",
        "        if 'Writer' in people:\n",
        "            writer = name.group(1).replace(\"'\", \"\")\n",
        "            writers.append(writer)\n",
        "\n",
        "        if len(writers) > 0:\n",
        "          movie_writers[row['id']] = writers\n",
        "        else:\n",
        "          movie_writers[row['id']] = ''\n",
        "\n",
        "        if len(directors) > 0:\n",
        "          movie_directors[row['id']] = directors\n",
        "        else:\n",
        "          movie_directors[row['id']] = ''\n",
        "\n",
        "writers = list(chain(movie_writers.values()))\n",
        "directors = list(chain(movie_directors.values()))\n",
        "movie_ids = list(movie_writers.keys())\n",
        "\n",
        "ext_df = pd.DataFrame(data={'id': movie_ids, 'writers': writers, 'directors': directors})\n",
        "\n",
        "ext_df[\"writers\"] = [','.join(map(str, x)) for x in ext_df['writers']]\n",
        "ext_df[\"directors\"] = [','.join(map(str, x)) for x in ext_df['directors']]\n",
        "ext_df[\"id\"] = ext_df[\"id\"].astype(int)\n",
        "\n",
        "ext_df_spark = spark.createDataFrame(ext_df, \"movie_id: int, writers: string, directors: string\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ext_df_spark.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in ext_df_spark.columns]).show()"
      ],
      "metadata": {
        "id": "VsyFO281UOrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9m4P4OUDslr"
      },
      "outputs": [],
      "source": [
        "ext_meta = pd.read_csv(\"drive/MyDrive/Bigdata-Grp19/ext data/movies_metadata.csv\", low_memory=False)\n",
        "ext_meta = ext_meta.loc[:, ['id', 'imdb_id', 'vote_average', 'vote_count']]\n",
        "\n",
        "ext_meta['id'] = ext_meta['id'].apply(lambda x: x.replace('-', ''))\n",
        "ext_meta[\"id\"] = ext_meta[\"id\"].astype(int)\n",
        "\n",
        "ext_meta[\"imdb_id\"] = ext_meta[\"imdb_id\"].astype(str)\n",
        "ext_meta[\"vote_average\"] = ext_meta[\"vote_average\"].astype(float)\n",
        "\n",
        "ext_meta[\"vote_count\"] = ext_meta[\"vote_count\"].astype(float)\n",
        "\n",
        "ext_meta_spark = spark.createDataFrame(ext_meta, \"m_id: int,  imdb_id: string, vote_average: float, vote_count: float\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIUTkjXGva7u"
      },
      "outputs": [],
      "source": [
        "# Writers, directors plus imdb_id from metadata\n",
        "ext_data_result = ext_meta_spark.join(ext_df_spark,\\\n",
        "                                    ext_df_spark.movie_id == ext_meta_spark.m_id, \"inner\") \\\n",
        "                                    .drop(ext_df_spark.movie_id)\\\n",
        "                                    .drop(ext_meta_spark.m_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ext_data_result.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in ext_data_result.columns]).show()"
      ],
      "metadata": {
        "id": "wMZuE2nVUlML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try: joining using pandas\n",
        "# Convert df to pandas-df\n",
        "df_pandas = df_train.toPandas()\n",
        "#print(df_pandas)\n",
        "df_train = df_train.join(ext_data_result, \\\n",
        "                          df_train[\"tconst\"] == ext_data_result[\"imdb_id\"],\"left\") \\\n",
        "                         .drop(ext_data_result.imdb_id)"
      ],
      "metadata": {
        "id": "IgnzozlKn-br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Writers directors on the df_train (join left to use original df as base)\n",
        "df_train = df_train.join(ext_data_result, \\\n",
        "                        df_train[\"tconst\"] == ext_data_result[\"imdb_id\"],\"left\") \\\n",
        "                       .drop(ext_data_result.imdb_id)\n",
        "\n"
      ],
      "metadata": {
        "id": "uhSrj7Io7Clu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_empty_votes_with_average_director(df):\n",
        "    # Add new column with average rate for director(s)\n",
        "    average_per_director = df.groupBy(\"directors\")\\\n",
        "                          .agg(F.avg(\"numVotes\")\\\n",
        "                          .alias(\"avg num votes per director\"))\n",
        "\n",
        "    # Join: directors names and averages on df\n",
        "    df = df.join(average_per_director,\\\n",
        "                            df.directors == average_per_director.directors,\\\n",
        "                            \"inner\")\\\n",
        "                            .drop(average_per_director.directors)\n",
        "\n",
        "    # Replce empty fields with that column\n",
        "    df = df.withColumn(\"numVotes\", \\\n",
        "                        F.when(df[\"numVotes\"] == 0, average_per_director[\"avg num votes per director\"])\\\n",
        "                        .otherwise(df[\"numVotes\"])) \\\n",
        "                        .drop(average_per_director[\"avg num votes per director\"])\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "L8Wn3wSbw7EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = replace_special_characters(df_train, 'directors')\n",
        "df_train = replace_special_characters(df_train, 'writers')\n",
        "df_train = replace_empty_votes_with_average_director(df_train)"
      ],
      "metadata": {
        "id": "-9ukV25CxL8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df_train.columns]).show() # Check if empty values\n"
      ],
      "metadata": {
        "id": "fQwSuFJtnNLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ39BhYvzs8J"
      },
      "source": [
        "## 1.3 Data Segregation \n",
        " \t\t\n",
        "Split subsets of data to train the model and further validate how it performs against new data (splitting it into training and evaluation subsets)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import FeatureHasher,Tokenizer,CountVectorizer,QuantileDiscretizer\n",
        "from pyspark.ml.feature import StringIndexer,VectorIndexer,MaxAbsScaler,StopWordsRemover\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
      ],
      "metadata": {
        "id": "6_CKFv6MOmeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a97On4LSr4re"
      },
      "outputs": [],
      "source": [
        "# scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "tokenizerTitle = Tokenizer(inputCol=\"primaryTitle\", outputCol=\"wordsPrimaryTitle\")\n",
        "stopRemover = StopWordsRemover(inputCol=\"wordsPrimaryTitle\", outputCol=\"wordsPrimaryTitleRemovedStop\")\n",
        "titleCV = CountVectorizer(inputCol=\"wordsPrimaryTitleRemovedStop\", outputCol=\"primaryTitleFeatures\")\n",
        "\n",
        "tokenizerDirectors = Tokenizer(inputCol=\"directors\", outputCol=\"wordsDirectors\")\n",
        "directorsCV = CountVectorizer(inputCol=\"wordsDirectors\", outputCol=\"directorsFeatures\")\n",
        "\n",
        "tokenizerWriters= Tokenizer(inputCol=\"writers\", outputCol=\"wordsWriters\")\n",
        "writersCV = CountVectorizer(inputCol=\"wordsWriters\", outputCol=\"writersFeatures\")\n",
        "\n",
        "discretizerStartYear = QuantileDiscretizer(numBuckets=20, inputCol=\"startYear\", outputCol=\"discStartYear\")\n",
        "\n",
        "discretizerRunTimeMinutes = QuantileDiscretizer(numBuckets=3, inputCol=\"runtimeMinutes\", outputCol=\"discRuntimeMinutes\")\n",
        "\n",
        "discretizerNumVotes = QuantileDiscretizer(numBuckets=4, inputCol=\"numVotes\", outputCol=\"discNumVotes\")\n",
        "\n",
        "assembler = VectorAssembler( inputCols=[\"primaryTitleFeatures\",\"discRuntimeMinutes\",\"discNumVotes\",\"discStartYear\",\"runtimeMinutes\",\"numVotes\",\"startYear\",\"endYear\"],outputCol=\"features\")\n",
        "\n",
        "# Train a GBT model.\n",
        "df_train = df_train.withColumn(\"label\", df_train.label.cast(T.IntegerType()))\n",
        "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "\n",
        "#pipeline = Pipeline(stages=[assembler,featureIndexer, gbt])\n",
        "#pipeline = Pipeline(stages=[  tokenizerDirectors, directorsCV, tokenizerWriters, writersCV, assembler, gbt])\n",
        "pipeline = Pipeline(stages=[ tokenizerTitle, stopRemover, titleCV, discretizerStartYear,discretizerRunTimeMinutes ,discretizerNumVotes, assembler, gbt])\n",
        "\n",
        "(trainingData, testData) = df_train.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Fit the pipeline\n",
        "model = pipeline.fit(trainingData)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG4GVwa_z8Mz"
      },
      "source": [
        "## 1.4 Data Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkjOAa4hsJeb"
      },
      "outputs": [],
      "source": [
        " # Select (prediction, true label) and compute accuracy , test error\n",
        "prediction = model.transform(testData)\n",
        "#evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\")\n",
        "\n",
        "accuracy = evaluator.evaluate(prediction)\n",
        "print(\"Accuracy = %g\" % (accuracy))\n",
        "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
        "\n",
        "#gbtModel = model.stages[1]\n",
        "#print(gbtModel)  # summary only"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator2 = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "accuracy2 = evaluator2.evaluate(prediction)\n",
        "print(\"Accuracy = %g\" % (accuracy2))\n",
        "print(\"Test Error = %g\" % (1.0 - accuracy2))"
      ],
      "metadata": {
        "id": "0ooYNRJTLDJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uoQrIScr8uJ"
      },
      "source": [
        "# 2. Analytics/Machine Learning\n",
        "*From Pramods notebook*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnP35-9txRzd"
      },
      "outputs": [],
      "source": [
        "model_all = pipeline.fit(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df_test.columns]).show()\n",
        "df_test = remove_empty(df_test, \"runTimeMinutes\", T.IntegerType())\n",
        "df_test = df_test.withColumn(\"runtimeMinutes\", \\\n",
        "                    F.when(df_test[\"runTimeMinutes\"].isNull(), 0)\\\n",
        "                    .otherwise(df_test[\"runTimeMinutes\"])) # REMOVE LAST RUNTIME MINUTE\n",
        "df_test.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df_test.columns]).show()\n"
      ],
      "metadata": {
        "id": "79oScKQYQNpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.dtypes"
      ],
      "metadata": {
        "id": "fjhABYW8f5gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test set and create output file.\n",
        "test_prediction = model_all.transform(df_test)\n",
        "out_test_df = test_prediction.withColumn(\"prediction\", test_prediction.prediction.cast(T.BooleanType()))\n",
        "out_test_df = out_test_df.select('prediction')\n",
        "out_test_df_pd = out_test_df.toPandas()\n"
      ],
      "metadata": {
        "id": "WuVsAoDXTE8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_validation.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df_validation.columns]).show()\n",
        "df_validation = df_validation.withColumn(\"runtimeMinutes\", \\\n",
        "                    F.when(df_validation[\"runTimeMinutes\"].isNull(), 0)\\\n",
        "                    .otherwise(df_validation[\"runTimeMinutes\"])) # REMOVE LAST RUNTIME MINUTE\n",
        "df_validation.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df_validation.columns]).show()\n"
      ],
      "metadata": {
        "id": "fzJ_dn1uYuka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_test_df_pd.to_csv(\"drive/MyDrive/Big Data - Grp19 IMDB/output/team-19-test-set-output.txt\", index=False,header=False)"
      ],
      "metadata": {
        "id": "bSyRINhSV655"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18J8di77peCn"
      },
      "outputs": [],
      "source": [
        "# Make predictions on validation set and create output file.\n",
        "valid_prediction = model_all.transform(df_validation)\n",
        "out_valid_df = valid_prediction.withColumn(\"prediction\",valid_prediction.prediction.cast(T.BooleanType()))\n",
        "out_valid_df = out_valid_df.select('prediction')\n",
        "out_valid_df_pd = out_valid_df.toPandas()\n",
        "\n",
        "out_valid_df_pd.to_csv(\"drive/MyDrive/Big Data - Grp19 IMDB/output/team-19-valid-set-output.txt\", index=False,header=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "B_qCfsl-GIH9",
        "Yx-Tm9WJyfXz",
        "gJ39BhYvzs8J",
        "jG4GVwa_z8Mz",
        "7uoQrIScr8uJ"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}